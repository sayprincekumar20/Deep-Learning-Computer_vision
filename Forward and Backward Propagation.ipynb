{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa1491f",
   "metadata": {},
   "source": [
    "  > **Topic Forward and Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b78caf",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of forward propagation in a neural network'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077fbaf",
   "metadata": {},
   "source": [
    "# Forward Propagation in a Neural Network\n",
    "\n",
    "**Forward propagation** is the process by which input data is passed through the layers of a neural network to produce an output. It is a key phase in the working of a neural network, where the network makes predictions based on the given input. Forward propagation involves the following steps:\n",
    "\n",
    "## 1. **Input Layer**\n",
    "\n",
    "- The process starts with the **input layer**, where the data (e.g., an image, text, or any other input) is fed into the network.\n",
    "- Each input feature corresponds to a node in the input layer.\n",
    "- In this layer, the input data is not modified or processed; it simply enters the network and moves to the next layer.\n",
    "\n",
    "## 2. **Weighted Sum of Inputs**\n",
    "\n",
    "- After the input layer, the data moves to the **first hidden layer**.\n",
    "- Each neuron in the hidden layer computes a **weighted sum** of the inputs it receives. This is achieved by multiplying each input by its corresponding **weight** and adding a **bias**.\n",
    "  \n",
    "  Mathematically, for each neuron:\n",
    "  \n",
    "  \\[\n",
    "  z = \\sum (x_i \\cdot w_i) + b\n",
    "  \\]\n",
    "  \n",
    "  Where:\n",
    "  - \\( x_i \\) is the input value for the \\(i\\)-th input.\n",
    "  - \\( w_i \\) is the weight associated with the \\(i\\)-th input.\n",
    "  - \\( b \\) is the bias term.\n",
    "\n",
    "## 3. **Activation Function**\n",
    "\n",
    "- The weighted sum \\( z \\) is then passed through an **activation function**. The activation function introduces non-linearity into the network, allowing it to learn complex patterns and relationships.\n",
    "- Common activation functions include:\n",
    "  - **ReLU** (Rectified Linear Unit)\n",
    "  - **Sigmoid**\n",
    "  - **Tanh**\n",
    "\n",
    "  Mathematically, the output of a neuron after applying the activation function is:\n",
    "\n",
    "  \\[\n",
    "  a = \\text{Activation}(z)\n",
    "  \\]\n",
    "  \n",
    "  Where:\n",
    "  - \\( a \\) is the activated output.\n",
    "  - **Activation** is the activation function (e.g., ReLU, sigmoid).\n",
    "\n",
    "## 4. **Propagation Through Layers**\n",
    "\n",
    "- This process of computing the weighted sum and passing it through an activation function is repeated for each subsequent layer of the network, including any **hidden layers** and the **output layer**.\n",
    "- Each hidden layer processes the data in a similar manner, but with its own set of weights, biases, and activation function.\n",
    "\n",
    "## 5. **Output Layer**\n",
    "\n",
    "- Finally, after passing through all the hidden layers, the data reaches the **output layer**, where the final prediction is made.\n",
    "- For a classification problem, the output is often a probability distribution (e.g., using the **softmax** activation function), and for a regression problem, it might be a continuous value (e.g., using a **linear activation function**).\n",
    "\n",
    "## 6. **Output**\n",
    "\n",
    "- The output of the network is used as the **prediction** or **decision**. In a classification problem, it could represent the likelihood of an input belonging to a particular class. In a regression problem, it could represent a predicted value.\n",
    "  \n",
    "### Example: Forward Propagation for a Simple Neural Network\n",
    "\n",
    "Consider a simple neural network with:\n",
    "- **Input layer**: 3 features (\\( x_1, x_2, x_3 \\)).\n",
    "- **Hidden layer**: 2 neurons.\n",
    "- **Output layer**: 1 neuron (binary classification).\n",
    "\n",
    "Let the input vector be \\( \\mathbf{x} = [x_1, x_2, x_3] \\), and the weights and biases are represented as:\n",
    "\n",
    "\\[\n",
    "\\mathbf{w_1} = [w_{11}, w_{12}, w_{13}], \\quad \\mathbf{w_2} = [w_{21}, w_{22}, w_{23}], \\quad \\mathbf{b_1, b_2}\n",
    "\\]\n",
    "\n",
    "1. Compute the weighted sum for the hidden layer neurons:\n",
    "\n",
    "\\[\n",
    "z_1 = (x_1 \\cdot w_{11} + x_2 \\cdot w_{12} + x_3 \\cdot w_{13}) + b_1\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "z_2 = (x_1 \\cdot w_{21} + x_2 \\cdot w_{22} + x_3 \\cdot w_{23}) + b_2\n",
    "\\]\n",
    "\n",
    "2. Apply activation functions (e.g., ReLU):\n",
    "\n",
    "\\[\n",
    "a_1 = \\text{ReLU}(z_1), \\quad a_2 = \\text{ReLU}(z_2)\n",
    "\\]\n",
    "\n",
    "3. Compute the weighted sum for the output neuron:\n",
    "\n",
    "\\[\n",
    "z_{\\text{out}} = (a_1 \\cdot w_{\\text{out1}} + a_2 \\cdot w_{\\text{out2}}) + b_{\\text{out}}\n",
    "\\]\n",
    "\n",
    "4. Apply the activation function (e.g., sigmoid for binary classification):\n",
    "\n",
    "\\[\n",
    "a_{\\text{out}} = \\text{Sigmoid}(z_{\\text{out}})\n",
    "\\]\n",
    "\n",
    "The final output \\( a_{\\text{out}} \\) is the prediction of the neural network.\n",
    "\n",
    "## 7. **Summary**\n",
    "\n",
    "- **Forward propagation** is the process by which data is passed through the layers of the neural network, transformed at each step, and ultimately produces an output.\n",
    "- It involves the **weighted sum** of inputs, the application of an **activation function**, and the propagation of the result through the network until reaching the output layer.\n",
    "- Forward propagation allows the neural network to make predictions and is an essential part of both **training** (during backpropagation) and **inference** (when using a trained model for prediction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8cd88",
   "metadata": {},
   "source": [
    "## Q2. What is the purpose of the activation function in forward propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72744fb4",
   "metadata": {},
   "source": [
    "# Purpose of the Activation Function in Forward Propagation\n",
    "\n",
    "The **activation function** plays a critical role in the forward propagation of a neural network by introducing **non-linearity** into the model. Here's an in-depth look at its purpose:\n",
    "\n",
    "## 1. **Introducing Non-linearity**\n",
    "\n",
    "- Without activation functions, a neural network would be a series of linear transformations, which would mean that no matter how many layers the network has, it would behave like a single-layer linear model.\n",
    "- **Activation functions introduce non-linearity**, allowing the network to learn and approximate complex patterns, relationships, and functions.\n",
    "- Non-linearity enables the network to model complex tasks such as image recognition, natural language processing, and more, which would be impossible for a purely linear model.\n",
    "\n",
    "### Why is non-linearity important?\n",
    "- Most real-world data (e.g., images, speech, text) are non-linear. If we use only linear transformations, no matter how many layers we add, the network would still be limited to linear approximations of the data.\n",
    "- **Non-linearity** in the activation function enables the network to learn complex patterns by combining the outputs of multiple neurons in a non-linear way.\n",
    "\n",
    "## 2. **Enabling Complex Decision Boundaries**\n",
    "\n",
    "- Activation functions help the network create **complex decision boundaries**. \n",
    "- For instance, in a classification problem, a neural network with non-linear activation functions can separate classes in a non-linear manner, improving the network's ability to perform classification on more complicated datasets.\n",
    "\n",
    "## 3. **Control Over the Output Range**\n",
    "\n",
    "- Activation functions often control the **output range** of a neuron, shaping the network's response to inputs.\n",
    "  - For example, the **sigmoid** function squashes its input to a range between 0 and 1, which is useful for binary classification.\n",
    "  - The **tanh** function squashes the input to a range between -1 and 1, making it suitable for data that can be both positive and negative.\n",
    "  - The **ReLU** function outputs the input directly if it's positive, and zero if it's negative, which is effective in avoiding problems like vanishing gradients during training.\n",
    "\n",
    "## 4. **Preventing Vanishing/Exploding Gradients (in specific cases)**\n",
    "\n",
    "- Some activation functions like **ReLU** help mitigate the issue of vanishing gradients that can occur in deep networks. Without proper activation functions, gradients could become too small during backpropagation, making it hard for the network to learn.\n",
    "- Activation functions like **ReLU** prevent gradients from vanishing by providing a direct path for gradients to flow through the network during backpropagation.\n",
    "\n",
    "## 5. **Enabling Gradient Descent Optimization**\n",
    "\n",
    "- In forward propagation, the activation functionâ€™s output is used to compute the error or loss during the backward propagation phase.\n",
    "- The **derivative of the activation function** is used in the backpropagation algorithm to update the weights of the neurons effectively. Without an activation function, this process would not work as efficiently.\n",
    "  \n",
    "## 6. **Common Activation Functions**\n",
    "\n",
    "- **Sigmoid**: Maps the output to a range between 0 and 1. Commonly used for binary classification tasks.\n",
    "  \n",
    "  \\[\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]\n",
    "  \n",
    "- **Tanh**: Maps the output to a range between -1 and 1. Suitable for data with both positive and negative values.\n",
    "  \n",
    "  \\[\n",
    "  \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "  \\]\n",
    "  \n",
    "- **ReLU**: Outputs the input directly if positive, otherwise outputs zero. Widely used due to its simplicity and ability to avoid vanishing gradients.\n",
    "  \n",
    "  \\[\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  \\]\n",
    "\n",
    "- **Leaky ReLU**: Similar to ReLU but allows a small negative slope for negative values of \\(x\\), preventing dead neurons.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Leaky ReLU}(x) = \\max(\\alpha x, x)\n",
    "  \\]\n",
    "  where \\( \\alpha \\) is a small constant.\n",
    "\n",
    "## 7. **Summary**\n",
    "\n",
    "- The **activation function** introduces non-linearity into the network, allowing it to model complex relationships in the data.\n",
    "- It helps the network create complex decision boundaries, making it more powerful for tasks such as classification and regression.\n",
    "- Activation functions also control the output range of neurons, prevent vanishing/exploding gradients, and play a crucial role in the **backpropagation** process during training.\n",
    "\n",
    "In essence, the activation function is essential for enabling a neural network to solve complex tasks that involve intricate patterns and relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6380ce4",
   "metadata": {},
   "source": [
    "## Q3. Describe the steps involved in the backward propagation (backpropagation) algorithm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dde480",
   "metadata": {},
   "source": [
    "# Steps in the Backpropagation Algorithm\n",
    "\n",
    "Backpropagation is a key algorithm used for training artificial neural networks. It involves updating the weights of the network by calculating the gradients of the loss function with respect to each weight using the chain rule of calculus. This is done in two main phases: the **forward pass** and the **backward pass**.\n",
    "\n",
    "## 1. **Forward Pass**\n",
    "Before we dive into backpropagation, itâ€™s important to know that the **forward pass** occurs first. In this phase:\n",
    "- The input data is passed through the network.\n",
    "- Each neuron computes a weighted sum of its inputs, applies an activation function, and produces an output.\n",
    "- This process continues through all layers until the final output is produced.\n",
    "\n",
    "The output of the forward pass is used to calculate the **loss**, which quantifies how far the model's prediction is from the actual target.\n",
    "\n",
    "## 2. **Compute the Loss**\n",
    "- The **loss function** calculates the difference between the networkâ€™s predicted output and the actual target value.\n",
    "- Common loss functions include:\n",
    "  - **Mean Squared Error (MSE)** for regression.\n",
    "  - **Cross-Entropy** for classification.\n",
    "\n",
    "Mathematically, for a single training example:\n",
    "\\[\n",
    "\\text{Loss} = L(\\hat{y}, y)\n",
    "\\]\n",
    "where:\n",
    "- \\( \\hat{y} \\) is the predicted output.\n",
    "- \\( y \\) is the true target value.\n",
    "\n",
    "## 3. **Backward Pass: Calculating Gradients**\n",
    "\n",
    "In the backward pass, we calculate the **gradients** of the loss function with respect to each weight and bias in the network. The goal is to adjust the weights to minimize the loss. This is done using the **chain rule**.\n",
    "\n",
    "### Step 1: Compute the Gradient of the Loss with Respect to the Output\n",
    "\n",
    "- Start by calculating the gradient of the loss function with respect to the output of the network.\n",
    "- For each output neuron, the derivative of the loss function with respect to its output is computed.\n",
    "  \n",
    "  \\[\n",
    "  \\frac{\\partial L}{\\partial \\hat{y}}\n",
    "  \\]\n",
    "\n",
    "### Step 2: Compute Gradients for Each Layer Using the Chain Rule\n",
    "\n",
    "- For each neuron in the network, compute how much the loss changes with respect to its **weights** and **biases**.\n",
    "- This requires applying the **chain rule** to propagate the error backward from the output layer to the input layer.\n",
    "  \n",
    "For a neuron \\( k \\) in a layer:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_k} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_k}\n",
    "\\]\n",
    "Where:\n",
    "- \\( z_k \\) is the weighted sum of inputs to the neuron.\n",
    "- \\( \\hat{y} \\) is the output of the neuron.\n",
    "\n",
    "The gradient of the loss with respect to the weight \\( w_k \\) of the neuron is calculated as:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial w_k}\n",
    "\\]\n",
    "\n",
    "The gradient of the loss with respect to the bias \\( b_k \\) is:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial b_k} = \\frac{\\partial L}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial b_k}\n",
    "\\]\n",
    "\n",
    "### Step 3: Propagate the Gradients Backward Through the Layers\n",
    "\n",
    "- After computing the gradients for each neuron in the output layer, these gradients are propagated backward through the hidden layers using the chain rule.\n",
    "- This process involves computing how much each neuron's output affects the subsequent layers.\n",
    "\n",
    "For hidden layer \\( j \\), the gradient of the loss with respect to \\( z_j \\) (the weighted sum of inputs) is:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_j} = \\sum_k \\frac{\\partial L}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j}\n",
    "\\]\n",
    "where \\( a_j \\) is the output of neuron \\( j \\), and \\( k \\) represents the neurons in the subsequent layer.\n",
    "\n",
    "### Step 4: Compute the Gradients for the Weights and Biases in Each Layer\n",
    "\n",
    "- Using the gradients from the previous step, compute the gradients of the weights and biases for each layer.\n",
    "- These gradients represent how much each weight and bias contributes to the loss.\n",
    "\n",
    "For the weights \\( w_j \\) in the hidden layer:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial L}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_j}\n",
    "\\]\n",
    "\n",
    "For the biases \\( b_j \\):\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial b_j} = \\frac{\\partial L}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial b_j}\n",
    "\\]\n",
    "\n",
    "### Step 5: Update the Weights and Biases\n",
    "\n",
    "- After computing the gradients, the weights and biases are updated using an optimization algorithm like **Stochastic Gradient Descent (SGD)**.\n",
    "- The update rule is:\n",
    "  \\[\n",
    "  w_j = w_j - \\eta \\cdot \\frac{\\partial L}{\\partial w_j}\n",
    "  \\]\n",
    "  \\[\n",
    "  b_j = b_j - \\eta \\cdot \\frac{\\partial L}{\\partial b_j}\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( \\eta \\) is the **learning rate**, a small positive constant that controls the size of the step taken during weight updates.\n",
    "\n",
    "### Step 6: Repeat the Process (Epochs)\n",
    "\n",
    "- The process of forward propagation, loss computation, backward propagation, and weight updates is repeated for multiple **epochs** (iterations over the entire training dataset).\n",
    "- Each epoch results in improved weights and a reduction in the loss function.\n",
    "\n",
    "## 4. **Summary of Backpropagation Steps**\n",
    "\n",
    "1. **Forward pass**: Pass input through the network, compute output, and calculate the loss.\n",
    "2. **Backward pass**: Compute gradients of the loss with respect to each weight and bias using the chain rule.\n",
    "3. **Weight update**: Update the weights and biases using the gradients computed in the backward pass.\n",
    "4. **Repeat**: Continue the process for multiple epochs to train the network.\n",
    "\n",
    "Backpropagation is the fundamental algorithm that allows neural networks to learn from data by iteratively improving the weights to minimize the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ceb6f",
   "metadata": {},
   "source": [
    "## Q4. What is the purpose of the chain rule in backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f69df",
   "metadata": {},
   "source": [
    "# Purpose of the Chain Rule in Backpropagation\n",
    "\n",
    "In the **backpropagation** algorithm, the **chain rule** of calculus plays a crucial role in efficiently computing the gradients of the loss function with respect to the weights and biases of the neural network. These gradients are necessary to update the weights during training. The chain rule allows us to propagate errors backward through the network in a systematic way.\n",
    "\n",
    "## 1. **Gradient Computation in Neural Networks**\n",
    "\n",
    "- Backpropagation involves calculating how much each weight in the network contributes to the overall error or loss. This is done by calculating the **gradient of the loss function** with respect to each weight and bias.\n",
    "- The gradient of the loss function tells us the direction and magnitude by which each weight and bias needs to be adjusted to minimize the error.\n",
    "  \n",
    "The chain rule is used to compute these gradients for each weight in each layer, starting from the output layer and propagating backward through the hidden layers.\n",
    "\n",
    "## 2. **How the Chain Rule Works in Backpropagation**\n",
    "\n",
    "In neural networks, each neuron performs a function that depends on the outputs of previous neurons. When we compute the error for each neuron, we need to find how much each part of the network (e.g., the weights and activations) influences the final error. The chain rule allows us to calculate this influence in a step-by-step manner.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Consider a loss function \\( L \\) that depends on the output \\( \\hat{y} \\), which in turn depends on intermediate values \\( z_1, z_2, \\dots, z_k \\) in the network. The chain rule helps us compute the derivative of \\( L \\) with respect to any intermediate variable or weight, given the dependencies between them.\n",
    "\n",
    "For example, if we want to compute the derivative of \\( L \\) with respect to a weight \\( w_k \\) in layer \\( k \\), the chain rule tells us that:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial w_k}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\frac{\\partial L}{\\partial \\hat{y}} \\) is the derivative of the loss with respect to the output.\n",
    "- \\( \\frac{\\partial \\hat{y}}{\\partial z_k} \\) is the derivative of the output with respect to the weighted sum of inputs to neuron \\( k \\).\n",
    "- \\( \\frac{\\partial z_k}{\\partial w_k} \\) is the derivative of the weighted sum of inputs with respect to the weight \\( w_k \\).\n",
    "\n",
    "### Step-by-Step Derivatives Using the Chain Rule\n",
    "\n",
    "The chain rule is applied iteratively for each layer of the network, starting from the output layer and moving backward through the network:\n",
    "\n",
    "1. **Output Layer**: Calculate the derivative of the loss function with respect to the output.\n",
    "2. **Hidden Layers**: For each hidden layer, compute how the loss changes with respect to the activations and weights in that layer.\n",
    "3. **Propagation of Gradients**: Use the chain rule to propagate these gradients back through the network, updating the weights and biases along the way.\n",
    "\n",
    "## 3. **Why is the Chain Rule Important in Backpropagation?**\n",
    "\n",
    "- **Efficient Gradient Calculation**: The chain rule enables efficient computation of gradients for all weights and biases in the network, even in deep networks with many layers.\n",
    "- **Allows Learning**: Without the chain rule, backpropagation wouldn't be able to update the weights based on the error for each layer. The chain rule makes it possible to \"break down\" the error into manageable parts and propagate it back through the network, which is crucial for **gradient descent** to update the weights correctly.\n",
    "- **Layer-by-Layer Gradient Propagation**: The chain rule allows for the propagation of the error layer by layer. The error at each layer is broken down into smaller pieces, reflecting how the current layer's weights and activations influence the overall error.\n",
    "  \n",
    "## 4. **Summary**\n",
    "\n",
    "- The **chain rule** is fundamental to the **backpropagation algorithm** as it enables the computation of gradients for each weight and bias in a neural network.\n",
    "- It allows us to decompose the error into smaller, manageable components and propagate them backward through the network to adjust the weights in a way that minimizes the loss.\n",
    "- By applying the chain rule iteratively from the output layer to the input layer, we can compute gradients efficiently and update the network's parameters to improve its performance.\n",
    "\n",
    "In essence, the chain rule allows backpropagation to perform its critical task: **learning from errors and adjusting weights accordingly**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61267b07",
   "metadata": {},
   "source": [
    "## Q5.Implement the forward propagation process for a simple neural network with one hidden layer using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22087c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Hidden Layer Output:\n",
      " [[0.63153712 0.60329049 0.66490264]\n",
      " [0.69870722 0.63782823 0.68515359]\n",
      " [0.72228788 0.75759132 0.66492812]\n",
      " [0.77871115 0.78351601 0.68517826]]\n",
      "Output Layer Output:\n",
      " [[0.6887684 ]\n",
      " [0.69568818]\n",
      " [0.70362112]\n",
      " [0.70932417]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function (for backpropagation purposes)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Example inputs (2 features)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Example outputs (for a binary classification problem)\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize the network's architecture (weights and biases)\n",
    "input_layer_size = 2   # 2 input features\n",
    "hidden_layer_size = 3  # 3 neurons in the hidden layer\n",
    "output_layer_size = 1  # 1 output neuron\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "np.random.seed(1)  # For reproducibility\n",
    "weights_input_hidden = np.random.rand(input_layer_size, hidden_layer_size)\n",
    "weights_hidden_output = np.random.rand(hidden_layer_size, output_layer_size)\n",
    "bias_hidden = np.random.rand(1, hidden_layer_size)\n",
    "bias_output = np.random.rand(1, output_layer_size)\n",
    "\n",
    "# Forward Propagation Process\n",
    "# Step 1: Calculate activations for the hidden layer\n",
    "hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden  # Linear transformation\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "# Step 2: Calculate activations for the output layer\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output  # Linear transformation\n",
    "output_layer_output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "# Print the results\n",
    "print(\"Input:\\n\", X)\n",
    "print(\"Hidden Layer Output:\\n\", hidden_layer_output)\n",
    "print(\"Output Layer Output:\\n\", output_layer_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f30748",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "**1. Sigmoid Activation Function:**\n",
    "\n",
    "- The sigmoid function maps values to a range between 0 and 1, which is useful for binary classification problems.\n",
    "- The sigmoid_derivative function is defined for potential use in backpropagation, although we won't use it here for forward propagation.\n",
    "\n",
    "**2. Network Initialization:**\n",
    "\n",
    "- We initialize the network with random weights for both the input-to-hidden layer and hidden-to-output layer. The biases for both layers are also randomly initialized.\n",
    "\n",
    "**3. Forward Propagation:**\n",
    "\n",
    "- The input data X (shape: 4 samples, 2 features) is passed through the network.\n",
    "- The hidden layer input is calculated by taking the dot product of X with the weights connecting the input layer to the hidden layer and adding the bias term.\n",
    "- The hidden layer output is obtained by applying the sigmoid activation function to the hidden layer input.\n",
    "- The output layer input is calculated by taking the dot product of the hidden layer output with the weights connecting the hidden layer to the output layer and adding the bias term.\n",
    "- The output layer output is obtained by applying the sigmoid activation function to the output layer input.\n",
    "\n",
    "**4. Results:**\n",
    "\n",
    "- The final outputs of the network after forward propagation are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6cea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
